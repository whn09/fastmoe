{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Cifar10 local training  \n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local` (or `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance).\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user has root access.\n",
      "nvidia-docker2 already installed. We are good to go!\n",
      "SageMaker instance route table setup is ok. We are good to go.\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "This notebook was created and tested on a single ml.p2.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-fastmoe'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 'file:///home/ec2-user/SageMaker/OpenNRE', 'pretrain': 'file:///home/ec2-user/SageMaker/OpenNRE/pretrain/baidubaike'}\n"
     ]
    }
   ],
   "source": [
    "# data_inputs = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/OpenNRE/benchmark/finre', bucket=bucket, key_prefix='data/finre')\n",
    "# pretrain_inputs = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/OpenNRE/pretrain/baidubaike', bucket=bucket, key_prefix='pretrain/baidubaike')\n",
    "data_inputs = 'file:///home/ec2-user/SageMaker/OpenNRE/benchmark/finre'\n",
    "pretrain_inputs = 'file:///home/ec2-user/SageMaker/OpenNRE/pretrain/baidubaike'\n",
    "inputs = {'training': data_inputs.replace('/benchmark/finre', ''), 'pretrain': pretrain_inputs}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/\n",
      "Creating lh84xoedo7-algo-1-o7s25 ... \n",
      "Creating lh84xoedo7-algo-1-o7s25 ... done\n",
      "Attaching to lh84xoedo7-algo-1-o7s25\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:40:12,811 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:40:12,836 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:40:12,840 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:40:26,906 sagemaker-training-toolkit INFO     Installing module with the following command:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m /opt/conda/bin/python -m pip install . -r requirements.txt\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting transformers==3.0.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[K     |████████████████████████████████| 769 kB 18.0 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting pytest==5.3.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\n",
      "\u001b[K     |████████████████████████████████| 234 kB 38.8 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting scikit-learn==0.22.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 38.6 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting scipy==1.4.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 46.0 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting nltk==3.4.5\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.35.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.16.3)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: botocore in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (1.19.3)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting certifi==2019.11.28\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 48.9 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: chardet==3.0.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (3.0.4)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting Click==7.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting idna==2.8\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting jmespath==0.9.4\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading jmespath-0.9.4-py2.py3-none-any.whl (24 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting joblib==0.14.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 48.5 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting numpy==1.18.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 39.8 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17)) (2.8.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting regex==2019.12.20\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading regex-2019.12.20-cp36-cp36m-manylinux2010_x86_64.whl (689 kB)\n",
      "\u001b[K     |████████████████████████████████| 689 kB 46.1 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting requests==2.22.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: s3transfer in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (0.3.3)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting sacremoses==0.0.38\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
      "\u001b[K     |████████████████████████████████| 860 kB 42.8 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting sentencepiece==0.1.85\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 39.5 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting six==1.13.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading six-1.13.0-py2.py3-none-any.whl (10 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting tqdm==4.41.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting urllib3==1.25.7\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading urllib3-1.25.7-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 47.2 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting sagemaker_containers\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading sagemaker_containers-2.8.6.post2.tar.gz (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 272 kB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting flask\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting filelock\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 44.6 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.2->-r requirements.txt (line 2)) (20.4)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.2->-r requirements.txt (line 2)) (0.7)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (2.0.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting more-itertools>=4.0.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading more_itertools-8.7.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting py>=1.5.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.5)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting pluggy<1.0,>=0.12\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting attrs>=17.4.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 8.2 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: pip in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (20.2.4)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting gunicorn\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 9.3 MB/s  eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hCollecting typing\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.3.3)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: gevent in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (20.9.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: inotify_simple==1.2.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.2.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: werkzeug>=0.15.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.0.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: paramiko>=2.4.2 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (2.7.2)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: psutil>=5.6.7 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (5.7.2)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (3.13.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting itsdangerous>=0.24\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting Jinja2>=2.10.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 43.6 MB/s eta 0:00:01\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.2->-r requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.2->-r requirements.txt (line 3)) (3.3.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.6/site-packages (from gunicorn->sagemaker_containers->-r requirements.txt (line 26)) (50.3.0.post20201006)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: greenlet>=0.4.17; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (0.4.17)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: zope.interface in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (5.1.2)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: zope.event in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (4.5.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: pynacl>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (1.4.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: bcrypt>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (3.2.0)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (3.1.1)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Collecting MarkupSafe>=0.23\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux2010_x86_64.whl (32 kB)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from pynacl>=1.0.1->paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (1.14.3)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (2.20)\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Building wheels for collected packages: nltk, sacremoses, sagemaker-containers, opennre, typing\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449905 sha256=314ecb00a069e44f68269ef9a1efe8fb9dbf3319c37530098ab3f3c941bb9d89\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/e3/c9/b0/ed26a73ef75a53145820825afa8e2d2c9b30fe9f6c10cd3202\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=bcdcbaac45065dbc8f27869f3c36dc96f7138b99535e61986cc58de860a27ba4\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Building wheel for sagemaker-containers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25h  Created wheel for sagemaker-containers: filename=sagemaker_containers-2.8.6.post2-cp36-cp36m-linux_x86_64.whl size=77619 sha256=0bfa366086ce7ff54e6d2bebfef06314ff46bf93e078bae469bba5a4def45fdd\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/68/04/ea/55bcff411d9f6eb0a064d2b1997dfd733640d8659ff2c98dbe\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Building wheel for opennre (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25h  Created wheel for opennre: filename=opennre-0.1-py3-none-any.whl size=47285 sha256=89f53173c5a1a263584f3a9421707ff2b9d4ae55210d1d0f188aa4a344645c1a\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-jcm7lcvw/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26308 sha256=eb720881527c089b3945e6d34ac610a959dd215af850e4c7c22b219dd14ba898\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/5f/63/c2/b85489bbea28cb5d36cfe197244f898428004fa3caa7a23116\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Successfully built nltk sacremoses sagemaker-containers opennre typing\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Installing collected packages: filelock, numpy, sentencepiece, tokenizers, tqdm, regex, six, Click, joblib, sacremoses, idna, certifi, urllib3, requests, transformers, more-itertools, py, pluggy, attrs, pytest, scipy, scikit-learn, nltk, jmespath, itsdangerous, MarkupSafe, Jinja2, flask, gunicorn, typing, sagemaker-containers, opennre\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: numpy\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: numpy 1.19.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling numpy-1.19.1:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled numpy-1.19.1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: tqdm\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: tqdm 4.46.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling tqdm-4.46.0:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled tqdm-4.46.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: six\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: six 1.15.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling six-1.15.0:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled six-1.15.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: Click\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: click 7.1.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling click-7.1.2:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled click-7.1.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: joblib\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: joblib 0.17.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling joblib-0.17.0:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled joblib-0.17.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: idna\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: idna 2.9\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling idna-2.9:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled idna-2.9\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: certifi\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: certifi 2020.6.20\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling certifi-2020.6.20:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled certifi-2020.6.20\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: urllib3\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: urllib3 1.25.11\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling urllib3-1.25.11:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled urllib3-1.25.11\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: requests\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: requests 2.24.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling requests-2.24.0:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled requests-2.24.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: scipy\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: scipy 1.5.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling scipy-1.5.2:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled scipy-1.5.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: scikit-learn\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: scikit-learn 0.23.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling scikit-learn-0.23.2:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled scikit-learn-0.23.2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m   Attempting uninstall: jmespath\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Found existing installation: jmespath 0.10.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     Uninstalling jmespath-0.10.0:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m       Successfully uninstalled jmespath-0.10.0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Successfully installed Click-7.0 Jinja2-2.11.3 MarkupSafe-1.1.1 attrs-20.3.0 certifi-2019.11.28 filelock-3.0.12 flask-1.1.2 gunicorn-20.0.4 idna-2.8 itsdangerous-1.1.0 jmespath-0.9.4 joblib-0.14.1 more-itertools-8.7.0 nltk-3.4.5 numpy-1.18.0 opennre-0.1 pluggy-0.13.1 py-1.10.0 pytest-5.3.2 regex-2019.12.20 requests-2.22.0 sacremoses-0.0.38 sagemaker-containers-2.8.6.post2 scikit-learn-0.22.1 scipy-1.4.1 sentencepiece-0.1.85 six-1.13.0 tokenizers-0.8.1rc1 tqdm-4.41.1 transformers-3.0.2 typing-3.7.4.3 urllib3-1.25.7\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:41:05,894 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Training Env:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"training\": \"/opt/ml/input/data/training\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"pretrain\": \"/opt/ml/input/data/pretrain\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     },\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"current_host\": \"algo-1-o7s25\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"algo-1-o7s25\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     ],\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"max_epoch\": 2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     },\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"training\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         },\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"pretrain\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         }\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     },\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"job_name\": \"pytorch-training-2021-02-24-01-39-10-418\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"master_hostname\": \"algo-1-o7s25\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-01-39-10-418/source/sourcedir.tar.gz\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"module_name\": \"train_test_finre_supervised_bert\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"current_host\": \"algo-1-o7s25\",\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m             \"algo-1-o7s25\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m         ]\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     },\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m     \"user_entry_point\": \"train_test_finre_supervised_bert.py\"\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m }\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Environment variables:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_HOSTS=[\"algo-1-o7s25\"]\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_HPS={\"max_epoch\":2}\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_USER_ENTRY_POINT=train_test_finre_supervised_bert.py\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-o7s25\",\"hosts\":[\"algo-1-o7s25\"]}\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_INPUT_DATA_CONFIG={\"pretrain\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_CHANNELS=[\"pretrain\",\"training\"]\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_CURRENT_HOST=algo-1-o7s25\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_MODULE_NAME=train_test_finre_supervised_bert\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-01-39-10-418/source/sourcedir.tar.gz\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"pretrain\":\"/opt/ml/input/data/pretrain\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-o7s25\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-o7s25\"],\"hyperparameters\":{\"max_epoch\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"pretrain\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-02-24-01-39-10-418\",\"log_level\":20,\"master_hostname\":\"algo-1-o7s25\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-01-39-10-418/source/sourcedir.tar.gz\",\"module_name\":\"train_test_finre_supervised_bert\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-o7s25\",\"hosts\":[\"algo-1-o7s25\"]},\"user_entry_point\":\"train_test_finre_supervised_bert.py\"}\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_USER_ARGS=[\"--max_epoch\",\"2\"]\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_CHANNEL_PRETRAIN=/opt/ml/input/data/pretrain\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m SM_HP_MAX_EPOCH=2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m /opt/conda/bin/python -m train_test_finre_supervised_bert --max_epoch 2\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m \n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 |\u001b[0m 2021-02-24 01:59:21,807 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mlh84xoedo7-algo-1-o7s25 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# git_config = {'repo': 'https://github.com/whn09/OpenNRE.git', 'branch': 'master'}\n",
    "hyperparameters = {'max_epoch': 2}\n",
    "\n",
    "estimator = PyTorch(entry_point='train_test_finre_supervised_bert.py',\n",
    "                            source_dir='.',\n",
    "                            #git_config=git_config,\n",
    "                            role=role,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            script_mode=True,\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type=instance_type)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 's3://sagemaker-us-east-1-579019700964/training', 'pretrain': 's3://sagemaker-us-east-1-579019700964/pretrain/baidubaike'}\n"
     ]
    }
   ],
   "source": [
    "data_inputs = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/OpenNRE/benchmark/finre', bucket=bucket, key_prefix='training/benchmark/finre')\n",
    "pretrain_inputs = sagemaker_session.upload_data(path='/home/ec2-user/SageMaker/OpenNRE/pretrain/baidubaike', bucket=bucket, key_prefix='pretrain/baidubaike')\n",
    "inputs = {'training': data_inputs.replace('/benchmark/finre', ''), 'pretrain': pretrain_inputs}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-24 04:59:49 Starting - Starting the training job...\n",
      "2021-02-24 05:00:10 Starting - Launching requested ML instancesProfilerReport-1614142727: InProgress\n",
      "......\n",
      "2021-02-24 05:01:14 Starting - Preparing the instances for training.........\n",
      "2021-02-24 05:02:33 Downloading - Downloading input data...\n",
      "2021-02-24 05:03:13 Training - Downloading the training image......\n",
      "2021-02-24 05:04:16 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-02-24 05:04:16,867 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-02-24 05:04:16,891 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-02-24 05:04:16,899 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-02-24 05:04:32,042 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mCollecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest==5.3.2\n",
      "  Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk==3.4.5\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.16.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (1.19.3)\u001b[0m\n",
      "\u001b[34mCollecting certifi==2019.11.28\n",
      "  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet==3.0.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (3.0.4)\u001b[0m\n",
      "\u001b[34mCollecting Click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna==2.8\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mCollecting jmespath==0.9.4\n",
      "  Downloading jmespath-0.9.4-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib==0.14.1\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.18.0\n",
      "  Downloading numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil==2.8.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 17)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting regex==2019.12.20\n",
      "  Downloading regex-2019.12.20-cp36-cp36m-manylinux2010_x86_64.whl (689 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 20)) (0.3.3)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses==0.0.38\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece==0.1.85\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting six==1.13.0\n",
      "  Downloading six-1.13.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.41.1\n",
      "  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3==1.25.7\n",
      "  Downloading urllib3-1.25.7-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker_containers\n",
      "  Downloading sagemaker_containers-2.8.6.post2.tar.gz (51 kB)\u001b[0m\n",
      "\u001b[34mCollecting flask\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.2->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==3.0.2->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting more-itertools>=4.0.0\n",
      "  Downloading more_itertools-8.7.0-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=17.4.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting py>=1.5.0\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pip in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (20.2.4)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gevent in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (20.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: inotify_simple==1.2.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.15.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: paramiko>=2.4.2 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (2.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.6.7 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (5.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker_containers->-r requirements.txt (line 26)) (3.13.0)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting Jinja2>=2.10.1\n",
      "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==3.0.2->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.2->-r requirements.txt (line 3)) (3.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.6/site-packages (from gunicorn->sagemaker_containers->-r requirements.txt (line 26)) (50.3.0.post20201006)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet>=0.4.17; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (0.4.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zope.event in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zope.interface in /opt/conda/lib/python3.6/site-packages (from gevent->sagemaker_containers->-r requirements.txt (line 26)) (5.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bcrypt>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynacl>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.6/site-packages (from paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (3.1.1)\u001b[0m\n",
      "\u001b[34mCollecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux2010_x86_64.whl (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.1 in /opt/conda/lib/python3.6/site-packages (from bcrypt>=3.1.3->paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (1.14.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4.2->sagemaker_containers->-r requirements.txt (line 26)) (2.20)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, sacremoses, sagemaker-containers, opennre, typing\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449903 sha256=9a3c1a7ff62e9ce5c9cf283eae61a3bdd8ef73e0b4376ab1675758fdf3955496\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/c9/b0/ed26a73ef75a53145820825afa8e2d2c9b30fe9f6c10cd3202\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=483a74a8d46261759e1ab930003de2d2708aa3f9f4963b8d3caa39a5379945c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "  Building wheel for sagemaker-containers (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker-containers (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-containers: filename=sagemaker_containers-2.8.6.post2-cp36-cp36m-linux_x86_64.whl size=77622 sha256=cb36ba6880f29de3984df28dc138e33ca3bc5da9c5f7f8008f1726275a0b9087\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/04/ea/55bcff411d9f6eb0a064d2b1997dfd733640d8659ff2c98dbe\n",
      "  Building wheel for opennre (setup.py): started\n",
      "  Building wheel for opennre (setup.py): finished with status 'done'\n",
      "  Created wheel for opennre: filename=opennre-0.1-py3-none-any.whl size=47285 sha256=2661e50a23c8bc937e8372b4ed87832ec90872c65132f777599410ebf1a9a7fb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-016jadl_/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\n",
      "  Building wheel for typing (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for typing (setup.py): finished with status 'done'\n",
      "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26308 sha256=76e8d37eeecf845c2a66638addd074be41eb67addc7c4e85557779035dfa4803\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/63/c2/b85489bbea28cb5d36cfe197244f898428004fa3caa7a23116\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk sacremoses sagemaker-containers opennre typing\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, six, Click, joblib, tqdm, sacremoses, tokenizers, numpy, sentencepiece, idna, certifi, urllib3, requests, filelock, transformers, pluggy, more-itertools, attrs, py, pytest, scipy, scikit-learn, nltk, jmespath, itsdangerous, MarkupSafe, Jinja2, flask, gunicorn, typing, sagemaker-containers, opennre\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: Click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 0.17.0\n",
      "    Uninstalling joblib-0.17.0:\n",
      "      Successfully uninstalled joblib-0.17.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.46.0\n",
      "    Uninstalling tqdm-4.46.0:\n",
      "      Successfully uninstalled tqdm-4.46.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\u001b[0m\n",
      "\u001b[34m    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: idna\u001b[0m\n",
      "\u001b[34m    Found existing installation: idna 2.9\n",
      "    Uninstalling idna-2.9:\n",
      "      Successfully uninstalled idna-2.9\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.6.20\n",
      "    Uninstalling certifi-2020.6.20:\n",
      "      Successfully uninstalled certifi-2020.6.20\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed Click-7.0 Jinja2-2.11.3 MarkupSafe-1.1.1 attrs-20.3.0 certifi-2019.11.28 filelock-3.0.12 flask-1.1.2 gunicorn-20.0.4 idna-2.8 itsdangerous-1.1.0 jmespath-0.9.4 joblib-0.14.1 more-itertools-8.7.0 nltk-3.4.5 numpy-1.18.0 opennre-0.1 pluggy-0.13.1 py-1.10.0 pytest-5.3.2 regex-2019.12.20 requests-2.22.0 sacremoses-0.0.38 sagemaker-containers-2.8.6.post2 scikit-learn-0.22.1 scipy-1.4.1 sentencepiece-0.1.85 six-1.13.0 tokenizers-0.8.1rc1 tqdm-4.41.1 transformers-3.0.2 typing-3.7.4.3 urllib3-1.25.7\u001b[0m\n",
      "\u001b[34m2021-02-24 05:05:05,937 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"pretrain\": \"/opt/ml/input/data/pretrain\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_epoch\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"pretrain\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-02-24-04-58-47-294\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-04-58-47-294/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_test_finre_supervised_bert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_test_finre_supervised_bert.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max_epoch\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_test_finre_supervised_bert.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"pretrain\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"pretrain\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_test_finre_supervised_bert\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-04-58-47-294/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"pretrain\":\"/opt/ml/input/data/pretrain\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_epoch\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"pretrain\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-02-24-04-58-47-294\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-579019700964/pytorch-training-2021-02-24-04-58-47-294/source/sourcedir.tar.gz\",\"module_name\":\"train_test_finre_supervised_bert\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_test_finre_supervised_bert.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max_epoch\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_PRETRAIN=/opt/ml/input/data/pretrain\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_EPOCH=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m train_test_finre_supervised_bert --max_epoch 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.748 algo-1:80 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.749 algo-1:80 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.749 algo-1:80 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.749 algo-1:80 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.784 algo-1:80 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:19.785 algo-1:80 INFO hook.py:459] Hook is writing from the hook with pid: 80\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.744 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.744 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.744 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.744 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.745 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.754 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.754 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.754 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.754 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.758 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.758 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.758 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.758 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.759 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.761 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.761 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.762 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.762 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.766 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.766 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.766 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.766 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.767 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.769 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.769 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.769 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.769 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.773 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.774 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.774 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.774 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.774 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.776 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.777 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.777 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.777 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.781 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.781 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.781 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.781 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.782 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.784 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.784 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.784 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.784 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.788 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.788 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.788 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.788 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.789 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.791 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.791 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.791 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.791 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.795 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.796 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.796 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.796 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.797 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.803 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.803 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.803 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.803 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.808 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.808 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.808 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.808 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.809 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.811 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.811 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.811 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.811 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.816 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.816 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.816 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.816 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.817 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.823 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.823 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.823 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.823 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.827 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.827 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.827 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.827 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.828 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.830 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.830 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.830 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.831 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.835 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.835 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.835 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.835 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.836 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.843 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.843 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.843 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.843 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.847 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.847 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.847 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.847 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.848 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.850 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.850 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.850 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:05:20.850 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.465 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.466 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.466 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.466 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.466 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.467 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.467 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.467 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.467 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.468 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.469 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.469 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.469 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.469 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.470 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.470 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.470 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.470 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.471 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.471 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.471 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.471 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.472 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.473 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.473 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.473 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.473 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.474 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.474 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.474 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.474 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.475 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.475 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.476 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.476 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.476 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.477 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.477 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.477 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.477 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.478 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.478 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.478 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.478 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.478 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.480 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.480 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.480 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.480 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.480 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.481 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.481 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.481 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.481 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.482 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.482 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.483 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.483 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.483 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.484 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.484 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.484 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.484 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.485 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.485 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.485 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.485 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.486 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.487 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.487 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.487 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.487 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.488 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.488 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.488 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.488 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.489 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.489 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.489 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.490 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.490 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.491 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.491 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.491 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.491 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.492 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.492 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.492 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.492 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.493 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.494 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.494 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.494 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.494 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.494 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.495 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.495 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.495 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.495 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.497 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.497 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.497 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.497 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.497 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.498 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.498 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.498 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-02-24 05:08:32.498 algo-1:80 WARNING hook.py:944] var is not Tensor or list or tuple of Tensors, module_name:module.sentence_encoder.bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m2021-02-24 05:08:37,044 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-02-24 05:09:21 Uploading - Uploading generated training model\n",
      "2021-02-24 05:10:21 Completed - Training job completed\n",
      "Training seconds: 472\n",
      "Billable seconds: 472\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {'max_epoch': 2}\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "estimator = PyTorch(entry_point='train_test_finre_supervised_bert.py',\n",
    "                            source_dir='.',\n",
    "                            role=role,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            script_mode=True,\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type=instance_type)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint (in this case locally) which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "# from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# pytorch_model = PyTorchModel(model_data='s3://{}/sagemaker/DEMO-pytorch-mnist/model.tar.gz'.format(bucket), role=role,\n",
    "#                              entry_point='train_test_finre_supervised_bert.py', framework_version='1.6.0', py_version='py3')\n",
    "\n",
    "# predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "# predictor.accept = 'application/json'\n",
    "# predictor.content_type = 'application/json'\n",
    "\n",
    "# predictor.serializer = json_serializer\n",
    "# predictor.deserializer = json_deserializer\n",
    "\n",
    "# data = {\"token\": [\"<N>\", \"日\", \",\", \"“\", \"借\", \"壳\", \"”\", \"华\", \"盛\", \"达\", \"的\", \"刚\", \"泰\", \"控\", \"股\", \"在\", \"上\", \"交\", \"所\", \"举\", \"行\", \"了\", \"更\", \"名\", \"及\", \"上\", \"市\", \"仪\", \"式\", \"。\"], \"h\": {\"name\": \"华盛达\", \"id\": \"\", \"pos\": [7, 10]}, \"t\": {\"name\": \"刚泰控股\", \"id\": \"\", \"pos\": [11, 15]}, \"relation\": \"拥有\"}\n",
    "\n",
    "# outputs = predictor.predict(data)\n",
    "\n",
    "# print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
